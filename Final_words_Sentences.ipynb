{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eY0J5vAoTXhY",
    "outputId": "4b3b1f79-4844-4627-96d2-99f795ea515f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "2/2 [==============================] - 4s 9ms/step - loss: 3.2179 - accuracy: 0.0595\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.2121 - accuracy: 0.2262\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.2057 - accuracy: 0.2262\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.1989 - accuracy: 0.2262\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.1910 - accuracy: 0.2262\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.1817 - accuracy: 0.2262\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.1703 - accuracy: 0.2262\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.1558 - accuracy: 0.2262\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.1391 - accuracy: 0.2262\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.1168 - accuracy: 0.2262\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.0873 - accuracy: 0.2262\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.0524 - accuracy: 0.2262\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.0089 - accuracy: 0.2262\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.9517 - accuracy: 0.2262\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.8846 - accuracy: 0.2262\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.8109 - accuracy: 0.2262\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.7690 - accuracy: 0.2262\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.7474 - accuracy: 0.2262\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.7519 - accuracy: 0.2262\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7503 - accuracy: 0.2262\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7302 - accuracy: 0.2262\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.7014 - accuracy: 0.2262\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.6707 - accuracy: 0.2262\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.6559 - accuracy: 0.2262\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.6422 - accuracy: 0.2262\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6346 - accuracy: 0.2262\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6279 - accuracy: 0.2262\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6195 - accuracy: 0.2262\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6100 - accuracy: 0.2262\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6019 - accuracy: 0.2262\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5925 - accuracy: 0.2262\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5865 - accuracy: 0.2262\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5781 - accuracy: 0.2262\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.5707 - accuracy: 0.2262\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5644 - accuracy: 0.2262\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5576 - accuracy: 0.2262\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5507 - accuracy: 0.2262\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5435 - accuracy: 0.2262\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5375 - accuracy: 0.2262\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.5296 - accuracy: 0.2262\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.5227 - accuracy: 0.2262\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5149 - accuracy: 0.2381\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.5065 - accuracy: 0.2381\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4982 - accuracy: 0.2500\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.4898 - accuracy: 0.2857\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4801 - accuracy: 0.2976\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4717 - accuracy: 0.3214\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.4614 - accuracy: 0.3452\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.4512 - accuracy: 0.3571\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4417 - accuracy: 0.3452\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.4299 - accuracy: 0.3571\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4193 - accuracy: 0.3571\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.4090 - accuracy: 0.3571\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.3958 - accuracy: 0.3571\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.3833 - accuracy: 0.3571\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.3704 - accuracy: 0.3690\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.3561 - accuracy: 0.3571\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.3407 - accuracy: 0.3571\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.3288 - accuracy: 0.3571\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.3129 - accuracy: 0.3571\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.2969 - accuracy: 0.3690\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.2817 - accuracy: 0.3690\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.2627 - accuracy: 0.3810\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.2469 - accuracy: 0.3929\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.2282 - accuracy: 0.3810\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.2065 - accuracy: 0.3929\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.1868 - accuracy: 0.3810\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.1641 - accuracy: 0.3810\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.1409 - accuracy: 0.3929\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.1182 - accuracy: 0.4167\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.0944 - accuracy: 0.4167\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.0683 - accuracy: 0.4167\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.0438 - accuracy: 0.4286\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.0240 - accuracy: 0.4167\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.9967 - accuracy: 0.4286\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 1.9667 - accuracy: 0.4405\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.9426 - accuracy: 0.4286\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.9171 - accuracy: 0.4524\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.8894 - accuracy: 0.4762\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.8639 - accuracy: 0.4524\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.8384 - accuracy: 0.4762\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.8141 - accuracy: 0.4881\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.7999 - accuracy: 0.4881\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.7679 - accuracy: 0.5000\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.7475 - accuracy: 0.5000\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.7359 - accuracy: 0.4881\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.7011 - accuracy: 0.5119\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.6794 - accuracy: 0.5357\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.6525 - accuracy: 0.5476\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.6260 - accuracy: 0.5238\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.6022 - accuracy: 0.5000\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.5762 - accuracy: 0.5357\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.5546 - accuracy: 0.5595\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.5247 - accuracy: 0.5595\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.5033 - accuracy: 0.5238\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.4810 - accuracy: 0.5357\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.4513 - accuracy: 0.5476\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.4293 - accuracy: 0.5476\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.4055 - accuracy: 0.5595\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.3794 - accuracy: 0.5714\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 1.3597 - accuracy: 0.5833\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.3406 - accuracy: 0.5833\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.3183 - accuracy: 0.5833\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.3030 - accuracy: 0.5714\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.2792 - accuracy: 0.5714\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.2649 - accuracy: 0.5833\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.2407 - accuracy: 0.6190\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.2301 - accuracy: 0.6071\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.2064 - accuracy: 0.6429\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.2208 - accuracy: 0.6310\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.1719 - accuracy: 0.6429\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.1985 - accuracy: 0.5952\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.1676 - accuracy: 0.6310\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.1414 - accuracy: 0.6429\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.1219 - accuracy: 0.6786\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.0983 - accuracy: 0.6667\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.1020 - accuracy: 0.6667\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.0762 - accuracy: 0.6667\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.0677 - accuracy: 0.6667\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.0516 - accuracy: 0.6905\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.0317 - accuracy: 0.6905\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.0220 - accuracy: 0.6905\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.0085 - accuracy: 0.6786\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.0015 - accuracy: 0.6786\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.9746 - accuracy: 0.7143\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.9848 - accuracy: 0.6905\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.9750 - accuracy: 0.6905\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.9478 - accuracy: 0.7143\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.9446 - accuracy: 0.7381\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.9383 - accuracy: 0.6905\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.9374 - accuracy: 0.7143\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.9102 - accuracy: 0.7262\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.8847 - accuracy: 0.7262\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.8878 - accuracy: 0.7262\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.8694 - accuracy: 0.7262\n",
      "Epoch 136/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.8523 - accuracy: 0.7619\n",
      "Epoch 137/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.8454 - accuracy: 0.7619\n",
      "Epoch 138/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.8362 - accuracy: 0.7500\n",
      "Epoch 139/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.8227 - accuracy: 0.7500\n",
      "Epoch 140/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.8085 - accuracy: 0.7381\n",
      "Epoch 141/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.8123 - accuracy: 0.7381\n",
      "Epoch 142/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.7963 - accuracy: 0.7500\n",
      "Epoch 143/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.7836 - accuracy: 0.7976\n",
      "Epoch 144/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.7753 - accuracy: 0.7857\n",
      "Epoch 145/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.7655 - accuracy: 0.8214\n",
      "Epoch 146/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.7536 - accuracy: 0.8333\n",
      "Epoch 147/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.7439 - accuracy: 0.8214\n",
      "Epoch 148/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.7363 - accuracy: 0.8333\n",
      "Epoch 149/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.7261 - accuracy: 0.8333\n",
      "Epoch 150/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7183 - accuracy: 0.8333\n",
      "Epoch 151/300\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.7094 - accuracy: 0.8452\n",
      "Epoch 152/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.7014 - accuracy: 0.8333\n",
      "Epoch 153/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6923 - accuracy: 0.8333\n",
      "Epoch 154/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6843 - accuracy: 0.8214\n",
      "Epoch 155/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6743 - accuracy: 0.8333\n",
      "Epoch 156/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6654 - accuracy: 0.8452\n",
      "Epoch 157/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6602 - accuracy: 0.8571\n",
      "Epoch 158/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6525 - accuracy: 0.8571\n",
      "Epoch 159/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6432 - accuracy: 0.8571\n",
      "Epoch 160/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6376 - accuracy: 0.8452\n",
      "Epoch 161/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6305 - accuracy: 0.8452\n",
      "Epoch 162/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6210 - accuracy: 0.8690\n",
      "Epoch 163/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6140 - accuracy: 0.8571\n",
      "Epoch 164/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6070 - accuracy: 0.8690\n",
      "Epoch 165/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6008 - accuracy: 0.8690\n",
      "Epoch 166/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.5939 - accuracy: 0.8810\n",
      "Epoch 167/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5849 - accuracy: 0.8810\n",
      "Epoch 168/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5777 - accuracy: 0.8810\n",
      "Epoch 169/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.5753 - accuracy: 0.8810\n",
      "Epoch 170/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5669 - accuracy: 0.8810\n",
      "Epoch 171/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5615 - accuracy: 0.8810\n",
      "Epoch 172/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.5547 - accuracy: 0.8810\n",
      "Epoch 173/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5486 - accuracy: 0.8929\n",
      "Epoch 174/300\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5432 - accuracy: 0.8929\n",
      "Epoch 175/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.5355 - accuracy: 0.8929\n",
      "Epoch 176/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5293 - accuracy: 0.8929\n",
      "Epoch 177/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.5232 - accuracy: 0.8929\n",
      "Epoch 178/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5167 - accuracy: 0.9048\n",
      "Epoch 179/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.5104 - accuracy: 0.9048\n",
      "Epoch 180/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5051 - accuracy: 0.9048\n",
      "Epoch 181/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.4997 - accuracy: 0.9048\n",
      "Epoch 182/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4945 - accuracy: 0.9048\n",
      "Epoch 183/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4883 - accuracy: 0.9048\n",
      "Epoch 184/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4831 - accuracy: 0.9048\n",
      "Epoch 185/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4775 - accuracy: 0.9048\n",
      "Epoch 186/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4724 - accuracy: 0.9048\n",
      "Epoch 187/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4669 - accuracy: 0.9167\n",
      "Epoch 188/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.4616 - accuracy: 0.9167\n",
      "Epoch 189/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.4573 - accuracy: 0.9167\n",
      "Epoch 190/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4525 - accuracy: 0.9167\n",
      "Epoch 191/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4472 - accuracy: 0.9167\n",
      "Epoch 192/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4419 - accuracy: 0.9167\n",
      "Epoch 193/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.4367 - accuracy: 0.9167\n",
      "Epoch 194/300\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4311 - accuracy: 0.9167\n",
      "Epoch 195/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4264 - accuracy: 0.9167\n",
      "Epoch 196/300\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.4216 - accuracy: 0.9167\n",
      "Epoch 197/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.4166 - accuracy: 0.9167\n",
      "Epoch 198/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.4119 - accuracy: 0.9524\n",
      "Epoch 199/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.4074 - accuracy: 0.9524\n",
      "Epoch 200/300\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.4038 - accuracy: 0.9643\n",
      "Epoch 201/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.3990 - accuracy: 0.9643\n",
      "Epoch 202/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.3940 - accuracy: 0.9524\n",
      "Epoch 203/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.3898 - accuracy: 0.9524\n",
      "Epoch 204/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.3848 - accuracy: 0.9405\n",
      "Epoch 205/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.3806 - accuracy: 0.9405\n",
      "Epoch 206/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.3763 - accuracy: 0.9405\n",
      "Epoch 207/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.3711 - accuracy: 0.9524\n",
      "Epoch 208/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.3654 - accuracy: 0.9524\n",
      "Epoch 209/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.3609 - accuracy: 0.9524\n",
      "Epoch 210/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.3565 - accuracy: 0.9524\n",
      "Epoch 211/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.3526 - accuracy: 0.9524\n",
      "Epoch 212/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.3478 - accuracy: 0.9524\n",
      "Epoch 213/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.3430 - accuracy: 0.9524\n",
      "Epoch 214/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.3390 - accuracy: 0.9524\n",
      "Epoch 215/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.3351 - accuracy: 0.9524\n",
      "Epoch 216/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.3302 - accuracy: 0.9524\n",
      "Epoch 217/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.3260 - accuracy: 0.9524\n",
      "Epoch 218/300\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3222 - accuracy: 0.9524\n",
      "Epoch 219/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.3178 - accuracy: 0.9524\n",
      "Epoch 220/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.3139 - accuracy: 0.9524\n",
      "Epoch 221/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.3100 - accuracy: 0.9524\n",
      "Epoch 222/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.3061 - accuracy: 0.9524\n",
      "Epoch 223/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.3025 - accuracy: 0.9524\n",
      "Epoch 224/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2987 - accuracy: 0.9524\n",
      "Epoch 225/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2945 - accuracy: 0.9524\n",
      "Epoch 226/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2905 - accuracy: 0.9524\n",
      "Epoch 227/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2869 - accuracy: 0.9643\n",
      "Epoch 228/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2834 - accuracy: 0.9643\n",
      "Epoch 229/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2794 - accuracy: 0.9643\n",
      "Epoch 230/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.2760 - accuracy: 0.9643\n",
      "Epoch 231/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2723 - accuracy: 0.9643\n",
      "Epoch 232/300\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.2685 - accuracy: 0.9643\n",
      "Epoch 233/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2651 - accuracy: 0.9643\n",
      "Epoch 234/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.2614 - accuracy: 0.9643\n",
      "Epoch 235/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.2579 - accuracy: 0.9643\n",
      "Epoch 236/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2544 - accuracy: 0.9643\n",
      "Epoch 237/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2507 - accuracy: 0.9643\n",
      "Epoch 238/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2473 - accuracy: 0.9643\n",
      "Epoch 239/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2433 - accuracy: 0.9643\n",
      "Epoch 240/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.2401 - accuracy: 0.9643\n",
      "Epoch 241/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2367 - accuracy: 0.9643\n",
      "Epoch 242/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2331 - accuracy: 0.9643\n",
      "Epoch 243/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2297 - accuracy: 0.9762\n",
      "Epoch 244/300\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.2259 - accuracy: 0.9762\n",
      "Epoch 245/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2231 - accuracy: 0.9762\n",
      "Epoch 246/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2202 - accuracy: 0.9643\n",
      "Epoch 247/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2167 - accuracy: 0.9643\n",
      "Epoch 248/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2138 - accuracy: 0.9643\n",
      "Epoch 249/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2104 - accuracy: 0.9881\n",
      "Epoch 250/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.2073 - accuracy: 0.9881\n",
      "Epoch 251/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.2044 - accuracy: 0.9881\n",
      "Epoch 252/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2015 - accuracy: 0.9881\n",
      "Epoch 253/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1983 - accuracy: 0.9881\n",
      "Epoch 254/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1955 - accuracy: 0.9881\n",
      "Epoch 255/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1925 - accuracy: 0.9881\n",
      "Epoch 256/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1893 - accuracy: 1.0000\n",
      "Epoch 257/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1863 - accuracy: 1.0000\n",
      "Epoch 258/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1830 - accuracy: 1.0000\n",
      "Epoch 259/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1803 - accuracy: 1.0000\n",
      "Epoch 260/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1776 - accuracy: 1.0000\n",
      "Epoch 261/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1747 - accuracy: 1.0000\n",
      "Epoch 262/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1719 - accuracy: 1.0000\n",
      "Epoch 263/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1691 - accuracy: 1.0000\n",
      "Epoch 264/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1669 - accuracy: 1.0000\n",
      "Epoch 265/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1640 - accuracy: 1.0000\n",
      "Epoch 266/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1618 - accuracy: 1.0000\n",
      "Epoch 267/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1590 - accuracy: 1.0000\n",
      "Epoch 268/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1566 - accuracy: 1.0000\n",
      "Epoch 269/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1545 - accuracy: 1.0000\n",
      "Epoch 270/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1520 - accuracy: 1.0000\n",
      "Epoch 271/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1494 - accuracy: 1.0000\n",
      "Epoch 272/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1471 - accuracy: 1.0000\n",
      "Epoch 273/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1450 - accuracy: 1.0000\n",
      "Epoch 274/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1431 - accuracy: 1.0000\n",
      "Epoch 275/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1411 - accuracy: 1.0000\n",
      "Epoch 276/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1393 - accuracy: 1.0000\n",
      "Epoch 277/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1371 - accuracy: 1.0000\n",
      "Epoch 278/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1350 - accuracy: 1.0000\n",
      "Epoch 279/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1330 - accuracy: 1.0000\n",
      "Epoch 280/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1306 - accuracy: 1.0000\n",
      "Epoch 281/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1289 - accuracy: 1.0000\n",
      "Epoch 282/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1271 - accuracy: 1.0000\n",
      "Epoch 283/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1253 - accuracy: 1.0000\n",
      "Epoch 284/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1235 - accuracy: 1.0000\n",
      "Epoch 285/300\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1216 - accuracy: 1.0000\n",
      "Epoch 286/300\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1198 - accuracy: 1.0000\n",
      "Epoch 287/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1182 - accuracy: 1.0000\n",
      "Epoch 288/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1164 - accuracy: 1.0000\n",
      "Epoch 289/300\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1149 - accuracy: 1.0000\n",
      "Epoch 290/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1134 - accuracy: 1.0000\n",
      "Epoch 291/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1117 - accuracy: 1.0000\n",
      "Epoch 292/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1103 - accuracy: 1.0000\n",
      "Epoch 293/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1089 - accuracy: 1.0000\n",
      "Epoch 294/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1077 - accuracy: 1.0000\n",
      "Epoch 295/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1064 - accuracy: 1.0000\n",
      "Epoch 296/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1048 - accuracy: 1.0000\n",
      "Epoch 297/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1034 - accuracy: 1.0000\n",
      "Epoch 298/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1022 - accuracy: 1.0000\n",
      "Epoch 299/300\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1005 - accuracy: 1.0000\n",
      "Epoch 300/300\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0993 - accuracy: 1.0000\n",
      "Model and tokenizers saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VENKATA REDDY\\envs\\testold\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Masking\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pickle\n",
    "\n",
    "# Define the dataset (input sequences and corresponding sentences)\n",
    "signs = [\n",
    "    [\"Hello\"],\n",
    "    [\"Hello\", \"How\", \"You\"],  # \"Hello, how are you?\"\n",
    "    [\"How\", \"You\"],  # \"How are you?\"\n",
    "    [\"I Love You\"],  # \"I love you.\"\n",
    "    [\"Thank\", \"You\"],  # \"Thank you!\"\n",
    "    [\"Hello\", \"Yes\"],  # \"Hello, yes!\"\n",
    "    [\"Iam\", \"Want\", \"Water\"],  # \"I want water.\"\n",
    "    [\"Iam\", \"Want\", \"Eat\"],  # \"I want to eat.\"\n",
    "    [\"You\", \"Eat\"],  # \"Are you eating?\"\n",
    "    [\"You\", \"Sleep\"],  # \"Are you sleeping?\"\n",
    "    [\"Iam\", \"Sleep\"],  # \"I am sleeping.\"\n",
    "    [\"Want\", \"Water\"],  # \"I need water.\"\n",
    "    [\"Want\", \"Eat\"],  # \"I need food.\"\n",
    "    [\"Iam\", \"Ok\"],  # \"I am okay.\"\n",
    "    [\"Ok\", \"Thank\", \"You\"],  # \"Okay, thank you!\"\n",
    "    [\"Hello\", \"Iam\"],  # \"Hello, I am here.\"\n",
    "    [\"How\", \"Iam\"],  # \"How am I?\"\n",
    "    [\"Want\", \"Sleep\"],  # \"I want to sleep.\"\n",
    "    [\"Beat\", \"You\"],  # \"I will beat you!\"\n",
    "    [\"Hello\", \"Thank\", \"You\"],  # \"Hello, thank you.\"\n",
    "    [\"You\", \"Want\", \"Water\"],  # \"Do you want water?\"\n",
    "]\n",
    "\n",
    "# Corresponding sentences\n",
    "sentences = [\n",
    "    \"Hello\",\n",
    "    \"Hello, how are you?\",\n",
    "    \"How are you?\",\n",
    "    \"I love you.\",\n",
    "    \"Thank you!\",\n",
    "    \"Hello, yes!\",\n",
    "    \"I want water.\",\n",
    "    \"I want to eat.\",\n",
    "    \"Are you eating?\",\n",
    "    \"Are you sleeping?\",\n",
    "    \"I am sleeping.\",\n",
    "    \"I need water.\",\n",
    "    \"I need food.\",\n",
    "    \"I am okay.\",\n",
    "    \"Okay, thank you!\",\n",
    "    \"Hello, I am here.\",\n",
    "    \"How am I?\",\n",
    "    \"I want to sleep.\",\n",
    "    \"I will beat you!\",\n",
    "    \"Hello, thank you.\",\n",
    "    \"Do you want water?\",\n",
    "]\n",
    "\n",
    "# Initialize Tokenizer and fit on the sign words\n",
    "input_tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "input_tokenizer.fit_on_texts(signs)\n",
    "\n",
    "# Encode the input sequences\n",
    "input_sequences = input_tokenizer.texts_to_sequences(signs)\n",
    "max_input_length = max(len(seq) for seq in input_sequences)\n",
    "input_sequences_padded = pad_sequences(input_sequences, maxlen=max_input_length, padding='post')\n",
    "\n",
    "# Prepare the output tokenizer and encode sentences\n",
    "output_tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "output_tokenizer.fit_on_texts(sentences)\n",
    "output_sequences = output_tokenizer.texts_to_sequences(sentences)\n",
    "max_output_length = max(len(seq) for seq in output_sequences)\n",
    "output_sequences_padded = pad_sequences(output_sequences, maxlen=max_output_length, padding='post')\n",
    "\n",
    "# Convert output sequences to categorical\n",
    "vocab_size_output = len(output_tokenizer.word_index) + 1\n",
    "output_sequences_categorical = np.array([to_categorical(seq, num_classes=vocab_size_output) for seq in output_sequences_padded])\n",
    "\n",
    "# Define the model architecture\n",
    "embedding_dim = 50  # Dimension of the embedding space\n",
    "latent_dim = 64  # Number of units in the LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(input_tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_input_length, mask_zero=True))\n",
    "# Change return_sequences to False to output a single hidden state vector\n",
    "model.add(LSTM(latent_dim))\n",
    "# Add a RepeatVector layer to repeat the hidden state vector to match the target sequence length\n",
    "model.add(keras.layers.RepeatVector(max_output_length))\n",
    "model.add(LSTM(latent_dim, return_sequences=True))\n",
    "model.add(Dense(vocab_size_output, activation='softmax'))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(input_sequences_padded, output_sequences_categorical, epochs=300, batch_size=16, verbose=1)\n",
    "\n",
    "# Save the model and tokenizers\n",
    "model.save('sentence_formation_model.h5')\n",
    "with open('input_tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(input_tokenizer, f)\n",
    "with open('output_tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(output_tokenizer, f)\n",
    "with open('max_seq_lengths.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'max_encoder_seq_length': max_input_length,\n",
    "        'max_decoder_seq_length': max_output_length,\n",
    "        'latent_dim': latent_dim\n",
    "    }, f)\n",
    "\n",
    "print(\"Model and tokenizers saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nwmAax_oc6_A",
    "outputId": "9a3519a8-2d7b-4713-ae10-246768c3799a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 898ms/step\n",
      "Signs: ['Hello', 'How', 'You'] -> Sentence: Hello how are you,\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Signs: ['Iam', 'Want', 'Water'] -> Sentence: I want water.\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Signs: ['I', 'Love', 'You'] -> Sentence: I you.\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Signs: ['You', 'Sleep'] -> Sentence: Are you sleeping?\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Signs: ['Want', 'Sleep'] -> Sentence: I want to sleep.\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Signs: ['Thank', 'You'] -> Sentence: Thank you.\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Signs: ['Hello', 'Iam'] -> Sentence: Hello i am here,\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Signs: ['Beat', 'You'] -> Sentence: I will beat you.\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Signs: ['Ok', 'Thank', 'You'] -> Sentence: Okay thank you.\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Signs: ['You', 'Want', 'Water'] -> Sentence: Do you want water?\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "# Load the trained model and tokenizers\n",
    "model = load_model('sentence_formation_model.h5')\n",
    "\n",
    "with open('input_tokenizer.pkl', 'rb') as f:\n",
    "    input_tokenizer = pickle.load(f)\n",
    "\n",
    "with open('output_tokenizer.pkl', 'rb') as f:\n",
    "    output_tokenizer = pickle.load(f)\n",
    "\n",
    "with open('max_seq_lengths.pkl', 'rb') as f:\n",
    "    seq_lengths = pickle.load(f)\n",
    "    max_encoder_seq_length = seq_lengths['max_encoder_seq_length']\n",
    "    max_decoder_seq_length = seq_lengths['max_decoder_seq_length']\n",
    "\n",
    "# Reverse the output tokenizer for decoding\n",
    "reverse_output_word_index = {v: k for k, v in output_tokenizer.word_index.items()}\n",
    "\n",
    "# Function to add punctuation based on sentence structure\n",
    "def add_punctuation(sentence):\n",
    "    if sentence.startswith(\"how\") or sentence.startswith(\"are\") or sentence.startswith(\"do\"):\n",
    "        return sentence.capitalize() + \"?\"\n",
    "    elif sentence.startswith(\"hello\"):\n",
    "        return sentence.capitalize() + \",\"\n",
    "    elif sentence.endswith(\"you\") or sentence.endswith(\"water\") or sentence.endswith(\"eat\") or sentence.endswith(\"sleep\"):\n",
    "        return sentence.capitalize() + \".\"\n",
    "    else:\n",
    "        return sentence.capitalize()\n",
    "\n",
    "# Function to predict a sentence based on input signs\n",
    "def predict_sentence(sign_sequence):\n",
    "    # Convert sign sequence to integers using the input tokenizer\n",
    "    input_sequence = input_tokenizer.texts_to_sequences([sign_sequence])\n",
    "    input_sequence_padded = pad_sequences(input_sequence, maxlen=max_encoder_seq_length, padding='post')\n",
    "\n",
    "    # Predict the output sequence\n",
    "    predictions = model.predict(input_sequence_padded)\n",
    "    output_sequence = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    # Decode the output sequence to words\n",
    "    decoded_sentence = []\n",
    "    for idx in output_sequence[0]:\n",
    "        if idx == 0:\n",
    "            break\n",
    "        word = reverse_output_word_index.get(idx, '')\n",
    "        decoded_sentence.append(word)\n",
    "\n",
    "    # Join words and add punctuation\n",
    "    sentence = ' '.join(decoded_sentence)\n",
    "    sentence_with_punctuation = add_punctuation(sentence)\n",
    "    return sentence_with_punctuation\n",
    "\n",
    "# Test cases (input as list of signs)\n",
    "test_cases = [\n",
    "    [\"Hello\", \"How\", \"You\"],         # \"Hello, how are you?\"\n",
    "    [\"Iam\", \"Want\", \"Water\"],        # \"I want water.\"\n",
    "    [\"I\", \"Love\", \"You\"],            # \"I love you.\"\n",
    "    [\"You\", \"Sleep\"],                # \"Are you sleeping?\"\n",
    "    [\"Want\", \"Sleep\"],               # \"I want to sleep.\"\n",
    "    [\"Thank\", \"You\"],                # \"Thank you!\"\n",
    "    [\"Hello\", \"Iam\"],                # \"Hello, I am here.\"\n",
    "    [\"Beat\", \"You\"],                 # \"I will beat you!\"\n",
    "    [\"Ok\", \"Thank\", \"You\"],          # \"Okay, thank you!\"\n",
    "    [\"You\", \"Want\", \"Water\"]         # \"Do you want water?\"\n",
    "]\n",
    "\n",
    "# Predict and display results for each test case\n",
    "for signs in test_cases:\n",
    "    predicted_sentence = predict_sentence(signs)\n",
    "    print(f\"Signs: {signs} -> Sentence: {predicted_sentence}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "testold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
